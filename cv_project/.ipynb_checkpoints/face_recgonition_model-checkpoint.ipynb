{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 360 images belonging to 9 classes.\n",
      "Found 36 images belonging to 9 classes.\n",
      "{'1': 0, '2': 1, '3': 2, '4': 3, '5': 4, '6': 5, '7': 6, '8': 7, '9': 8}\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " mobilenetv2_1.00_224 (Funct  (None, 7, 7, 1280)       2257984   \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 5, 5, 32)          368672    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 5, 5, 32)          0         \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 32)               0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 99        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,626,755\n",
      "Trainable params: 368,771\n",
      "Non-trainable params: 2,257,984\n",
      "_________________________________________________________________\n",
      "Number of trainable variables = 4\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002029D9C5798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002029D9C5798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": " logits and labels must be broadcastable: logits_size=[5,3] labels_size=[5,9]\n\t [[node categorical_crossentropy/softmax_cross_entropy_with_logits\n (defined at C:\\Users\\kopal\\Anaconda3\\lib\\site-packages\\keras\\backend.py:5010)\n]] [Op:__inference_train_function_8465]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node categorical_crossentropy/softmax_cross_entropy_with_logits:\nIn[0] categorical_crossentropy/softmax_cross_entropy_with_logits/Reshape:\t\nIn[1] categorical_crossentropy/softmax_cross_entropy_with_logits/Reshape_1:\n\nOperation defined at: (most recent call last)\n>>>   File \"C:\\Users\\kopal\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n>>>     \"__main__\", mod_spec)\n>>> \n>>>   File \"C:\\Users\\kopal\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n>>>     exec(code, run_globals)\n>>> \n>>>   File \"C:\\Users\\kopal\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n>>>     app.launch_new_instance()\n>>> \n>>>   File \"C:\\Users\\kopal\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 664, in launch_instance\n>>>     app.start()\n>>> \n>>>   File \"C:\\Users\\kopal\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 563, in start\n>>>     self.io_loop.start()\n>>> \n>>>   File \"C:\\Users\\kopal\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 148, in start\n>>>     self.asyncio_loop.run_forever()\n>>> \n>>>   File \"C:\\Users\\kopal\\Anaconda3\\lib\\asyncio\\base_events.py\", line 534, in run_forever\n>>>     self._run_once()\n>>> \n>>>   File \"C:\\Users\\kopal\\Anaconda3\\lib\\asyncio\\base_events.py\", line 1771, in _run_once\n>>>     handle._run()\n>>> \n>>>   File \"C:\\Users\\kopal\\Anaconda3\\lib\\asyncio\\events.py\", line 88, in _run\n>>>     self._context.run(self._callback, *self._args)\n>>> \n>>>   File \"C:\\Users\\kopal\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 690, in <lambda>\n>>>     lambda f: self._run_callback(functools.partial(callback, future))\n>>> \n>>>   File \"C:\\Users\\kopal\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 743, in _run_callback\n>>>     ret = callback()\n>>> \n>>>   File \"C:\\Users\\kopal\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 787, in inner\n>>>     self.run()\n>>> \n>>>   File \"C:\\Users\\kopal\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 748, in run\n>>>     yielded = self.gen.send(value)\n>>> \n>>>   File \"C:\\Users\\kopal\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 365, in process_one\n>>>     yield gen.maybe_future(dispatch(*args))\n>>> \n>>>   File \"C:\\Users\\kopal\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n>>>     yielded = next(result)\n>>> \n>>>   File \"C:\\Users\\kopal\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 272, in dispatch_shell\n>>>     yield gen.maybe_future(handler(stream, idents, msg))\n>>> \n>>>   File \"C:\\Users\\kopal\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n>>>     yielded = next(result)\n>>> \n>>>   File \"C:\\Users\\kopal\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 542, in execute_request\n>>>     user_expressions, allow_stdin,\n>>> \n>>>   File \"C:\\Users\\kopal\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n>>>     yielded = next(result)\n>>> \n>>>   File \"C:\\Users\\kopal\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 294, in do_execute\n>>>     res = shell.run_cell(code, store_history=store_history, silent=silent)\n>>> \n>>>   File \"C:\\Users\\kopal\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n>>>     return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\kopal\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2855, in run_cell\n>>>     raw_cell, store_history, silent, shell_futures)\n>>> \n>>>   File \"C:\\Users\\kopal\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in _run_cell\n>>>     return runner(coro)\n>>> \n>>>   File \"C:\\Users\\kopal\\Anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n>>>     coro.send(None)\n>>> \n>>>   File \"C:\\Users\\kopal\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3058, in run_cell_async\n>>>     interactivity=interactivity, compiler=compiler, result=result)\n>>> \n>>>   File \"C:\\Users\\kopal\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3249, in run_ast_nodes\n>>>     if (await self.run_code(code, result,  async_=asy)):\n>>> \n>>>   File \"C:\\Users\\kopal\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3326, in run_code\n>>>     exec(code_obj, self.user_global_ns, self.user_ns)\n>>> \n>>>   File \"<ipython-input-2-8099e8252abe>\", line 90, in <module>\n>>>     validation_data=val_generator)\n>>> \n>>>   File \"C:\\Users\\kopal\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\kopal\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1216, in fit\n>>>     tmp_logs = self.train_function(iterator)\n>>> \n>>>   File \"C:\\Users\\kopal\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 878, in train_function\n>>>     return step_function(self, iterator)\n>>> \n>>>   File \"C:\\Users\\kopal\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 867, in step_function\n>>>     outputs = model.distribute_strategy.run(run_step, args=(data,))\n>>> \n>>>   File \"C:\\Users\\kopal\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in run_step\n>>>     outputs = model.train_step(data)\n>>> \n>>>   File \"C:\\Users\\kopal\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 810, in train_step\n>>>     y, y_pred, sample_weight, regularization_losses=self.losses)\n>>> \n>>>   File \"C:\\Users\\kopal\\Anaconda3\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 201, in __call__\n>>>     loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n>>> \n>>>   File \"C:\\Users\\kopal\\Anaconda3\\lib\\site-packages\\keras\\losses.py\", line 141, in __call__\n>>>     losses = call_fn(y_true, y_pred)\n>>> \n>>>   File \"C:\\Users\\kopal\\Anaconda3\\lib\\site-packages\\keras\\losses.py\", line 245, in call\n>>>     return ag_fn(y_true, y_pred, **self._fn_kwargs)\n>>> \n>>>   File \"C:\\Users\\kopal\\Anaconda3\\lib\\site-packages\\keras\\losses.py\", line 1665, in categorical_crossentropy\n>>>     y_true, y_pred, from_logits=from_logits, axis=axis)\n>>> \n>>>   File \"C:\\Users\\kopal\\Anaconda3\\lib\\site-packages\\keras\\backend.py\", line 5010, in categorical_crossentropy\n>>>     labels=target, logits=output, axis=axis)\n>>> ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-8099e8252abe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     88\u001b[0m history = model.fit(train_generator,\n\u001b[0;32m     89\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m                     validation_data=val_generator)\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 59\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     60\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m:  logits and labels must be broadcastable: logits_size=[5,3] labels_size=[5,9]\n\t [[node categorical_crossentropy/softmax_cross_entropy_with_logits\n (defined at C:\\Users\\kopal\\Anaconda3\\lib\\site-packages\\keras\\backend.py:5010)\n]] [Op:__inference_train_function_8465]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node categorical_crossentropy/softmax_cross_entropy_with_logits:\nIn[0] categorical_crossentropy/softmax_cross_entropy_with_logits/Reshape:\t\nIn[1] categorical_crossentropy/softmax_cross_entropy_with_logits/Reshape_1:\n\nOperation defined at: (most recent call last)\n>>>   File \"C:\\Users\\kopal\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n>>>     \"__main__\", mod_spec)\n>>> \n>>>   File \"C:\\Users\\kopal\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n>>>     exec(code, run_globals)\n>>> \n>>>   File \"C:\\Users\\kopal\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n>>>     app.launch_new_instance()\n>>> \n>>>   File \"C:\\Users\\kopal\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 664, in launch_instance\n>>>     app.start()\n>>> \n>>>   File \"C:\\Users\\kopal\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 563, in start\n>>>     self.io_loop.start()\n>>> \n>>>   File \"C:\\Users\\kopal\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 148, in start\n>>>     self.asyncio_loop.run_forever()\n>>> \n>>>   File \"C:\\Users\\kopal\\Anaconda3\\lib\\asyncio\\base_events.py\", line 534, in run_forever\n>>>     self._run_once()\n>>> \n>>>   File \"C:\\Users\\kopal\\Anaconda3\\lib\\asyncio\\base_events.py\", line 1771, in _run_once\n>>>     handle._run()\n>>> \n>>>   File \"C:\\Users\\kopal\\Anaconda3\\lib\\asyncio\\events.py\", line 88, in _run\n>>>     self._context.run(self._callback, *self._args)\n>>> \n>>>   File \"C:\\Users\\kopal\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 690, in <lambda>\n>>>     lambda f: self._run_callback(functools.partial(callback, future))\n>>> \n>>>   File \"C:\\Users\\kopal\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 743, in _run_callback\n>>>     ret = callback()\n>>> \n>>>   File \"C:\\Users\\kopal\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 787, in inner\n>>>     self.run()\n>>> \n>>>   File \"C:\\Users\\kopal\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 748, in run\n>>>     yielded = self.gen.send(value)\n>>> \n>>>   File \"C:\\Users\\kopal\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 365, in process_one\n>>>     yield gen.maybe_future(dispatch(*args))\n>>> \n>>>   File \"C:\\Users\\kopal\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n>>>     yielded = next(result)\n>>> \n>>>   File \"C:\\Users\\kopal\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 272, in dispatch_shell\n>>>     yield gen.maybe_future(handler(stream, idents, msg))\n>>> \n>>>   File \"C:\\Users\\kopal\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n>>>     yielded = next(result)\n>>> \n>>>   File \"C:\\Users\\kopal\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 542, in execute_request\n>>>     user_expressions, allow_stdin,\n>>> \n>>>   File \"C:\\Users\\kopal\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n>>>     yielded = next(result)\n>>> \n>>>   File \"C:\\Users\\kopal\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 294, in do_execute\n>>>     res = shell.run_cell(code, store_history=store_history, silent=silent)\n>>> \n>>>   File \"C:\\Users\\kopal\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n>>>     return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\kopal\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2855, in run_cell\n>>>     raw_cell, store_history, silent, shell_futures)\n>>> \n>>>   File \"C:\\Users\\kopal\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in _run_cell\n>>>     return runner(coro)\n>>> \n>>>   File \"C:\\Users\\kopal\\Anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n>>>     coro.send(None)\n>>> \n>>>   File \"C:\\Users\\kopal\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3058, in run_cell_async\n>>>     interactivity=interactivity, compiler=compiler, result=result)\n>>> \n>>>   File \"C:\\Users\\kopal\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3249, in run_ast_nodes\n>>>     if (await self.run_code(code, result,  async_=asy)):\n>>> \n>>>   File \"C:\\Users\\kopal\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3326, in run_code\n>>>     exec(code_obj, self.user_global_ns, self.user_ns)\n>>> \n>>>   File \"<ipython-input-2-8099e8252abe>\", line 90, in <module>\n>>>     validation_data=val_generator)\n>>> \n>>>   File \"C:\\Users\\kopal\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\kopal\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1216, in fit\n>>>     tmp_logs = self.train_function(iterator)\n>>> \n>>>   File \"C:\\Users\\kopal\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 878, in train_function\n>>>     return step_function(self, iterator)\n>>> \n>>>   File \"C:\\Users\\kopal\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 867, in step_function\n>>>     outputs = model.distribute_strategy.run(run_step, args=(data,))\n>>> \n>>>   File \"C:\\Users\\kopal\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in run_step\n>>>     outputs = model.train_step(data)\n>>> \n>>>   File \"C:\\Users\\kopal\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 810, in train_step\n>>>     y, y_pred, sample_weight, regularization_losses=self.losses)\n>>> \n>>>   File \"C:\\Users\\kopal\\Anaconda3\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 201, in __call__\n>>>     loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n>>> \n>>>   File \"C:\\Users\\kopal\\Anaconda3\\lib\\site-packages\\keras\\losses.py\", line 141, in __call__\n>>>     losses = call_fn(y_true, y_pred)\n>>> \n>>>   File \"C:\\Users\\kopal\\Anaconda3\\lib\\site-packages\\keras\\losses.py\", line 245, in call\n>>>     return ag_fn(y_true, y_pred, **self._fn_kwargs)\n>>> \n>>>   File \"C:\\Users\\kopal\\Anaconda3\\lib\\site-packages\\keras\\losses.py\", line 1665, in categorical_crossentropy\n>>>     y_true, y_pred, from_logits=from_logits, axis=axis)\n>>> \n>>>   File \"C:\\Users\\kopal\\Anaconda3\\lib\\site-packages\\keras\\backend.py\", line 5010, in categorical_crossentropy\n>>>     labels=target, logits=output, axis=axis)\n>>> "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "IMAGE_SIZE = 224\n",
    "\n",
    "BATCH_SIZE = 5\n",
    "\n",
    "data_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    validation_split=0.2)\n",
    "\n",
    "train_generator = data_generator.flow_from_directory(\n",
    "    \"C:/Users/kopal/Documents/face_recognition_with_anti_spoofing/face_recognition_with_anti_spoofing/cv_project/train\",\n",
    "    target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    subset='training')\n",
    "\n",
    "val_generator = data_generator.flow_from_directory(\n",
    "    \"C:/Users/kopal/Documents/face_recognition_with_anti_spoofing/face_recognition_with_anti_spoofing/cv_project/test\",\n",
    "    target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    subset='validation')\n",
    "\n",
    "print(val_generator)\n",
    "\n",
    "for image_batch, label_batch in train_generator:\n",
    "    break\n",
    "\n",
    "print(train_generator.class_indices)\n",
    "\n",
    "labels = '\\n'.join(sorted(train_generator.class_indices.keys()))\n",
    "\n",
    "with open('labels.txt', 'w') as f:\n",
    "    f.write(labels)\n",
    "\n",
    "IMG_SHAPE = (IMAGE_SIZE, IMAGE_SIZE, 3)\n",
    "\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')\n",
    "\n",
    "base_model.trainable = False\n",
    "\n",
    "# Add a classification head\n",
    "# We are going to add more classification 'heads' to our model\n",
    "\n",
    "# 1 -   We are giving our base model (Top Layer removed, hidden and output layer UN-TRAINABLE)\n",
    "\n",
    "# 2 -   2D Convolution network (32 nodes, 3 Kernel size, Activation Function)\n",
    "#       [Remember how Convolutions are formed, with role of kernels]\n",
    "#       [ Kernels in Convents are Odd numbered]\n",
    "#       [ Kernels are just a determinant which is multiplied with Image Matrix]\n",
    "#       [They help in enhancement of features]\n",
    "\n",
    "# 3 -   We don't need all nodes, 20% of nodes will be dropped out [probability of each \"bad\" node being dropped is 20%\n",
    "#       Bad here means the nodes which are not contributing much value to the final output\n",
    "\n",
    "# 4 -   Convolution and pooling - 2X2 matrix is taken and a pooling is done\n",
    "#       Pooling is done for data size reduction by taking average\n",
    "\n",
    "# 5 -   All above are transformation layers, this is the main Dense Layer\n",
    "#       Dense layer takes input from all prev nodes and gives input to all next nodes.\n",
    "#       It is very densely connected and hence called the Dense Layer.\n",
    "#       We are using the Activation function of 'Softmax'\n",
    "#       There is another popular Activation function called 'Relu'\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    base_model,  # 1\n",
    "    tf.keras.layers.Conv2D(32, 3, activation='relu'),  # 2\n",
    "    tf.keras.layers.Dropout(0.2),  # 3\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),  # 4\n",
    "    tf.keras.layers.Dense(3, activation='softmax')  # 5\n",
    "])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),  # 1\n",
    "              loss='categorical_crossentropy',  # 2\n",
    "              metrics=['accuracy'])  # 3\n",
    "\n",
    "model.summary()\n",
    "\n",
    "print('Number of trainable variables = {}'.format(len(model.trainable_variables)))\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "history = model.fit(train_generator,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=val_generator)\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([min(plt.ylim()), 1])\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.ylim([0, 1.0])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()\n",
    "\n",
    "\"\"\"## Fine tuning\n",
    "In our feature extraction experiment, you were only training a few layers on top of an MobileNet V2 base model. The weights of the pre-trained network were **not** updated during training.\n",
    "\n",
    "One way to increase performance even further is to train (or \"fine-tune\") the weights of the top layers of the pre-trained model alongside the training of the classifier you added. The training process will force the weights to be tuned from generic features maps to features associated specifically to our dataset.\n",
    "\n",
    "### Un-freeze the top layers of the model\n",
    "\n",
    "All you need to do is unfreeze the `base_model` and set the bottom layers be un-trainable. Then, recompile the model (necessary for these changes to take effect), and resume training.\n",
    "\"\"\"\n",
    "\n",
    "base_model.trainable = True\n",
    "\n",
    "print(\"Number of layers in the base model: \", len(base_model.layers))\n",
    "\n",
    "fine_tune_at = 100\n",
    "\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-5),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "print('Number of trainable variables = {}'.format(len(model.trainable_variables)))\n",
    "\n",
    "history_fine = model.fit(train_generator,\n",
    "                         epochs=5,\n",
    "                         validation_data=val_generator\n",
    "                         )\n",
    "\n",
    "saved_model_dir = 'fine_tuning.h5'\n",
    "model.save(saved_model_dir)\n",
    "print(\"Model Saved to save/fine_tuning.h5\")\n",
    "\n",
    "#tf.saved_model.save(model, saved_model_dir)\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model_file(model)\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open('model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "\n",
    "\n",
    "acc = history_fine.history['accuracy']\n",
    "val_acc = history_fine.history['val_accuracy']\n",
    "\n",
    "loss = history_fine.history['loss']\n",
    "val_loss = history_fine.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([min(plt.ylim()), 1])\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.ylim([0, 1.0])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()\n",
    "\n",
    "'''\n",
    "# Summary:\n",
    "* **Using a pre-trained model for feature extraction**:  When working with a small dataset, it is common to take advantage of features learned by a model trained on a larger dataset in the same domain. This is done by instantiating the pre-trained model and adding a fully-connected classifier on top. The pre-trained model is \"frozen\" and only the weights of the classifier get updated during training.\n",
    "In this case, the convolutional base extracted all the features associated with each image and you just trained a classifier that determines the image class given that set of extracted features.\n",
    "\n",
    "* **Fine-tuning a pre-trained model**: To further improve performance, one might want to repurpose the top-level layers of the pre-trained models to the new dataset via fine-tuning.\n",
    "In this case, you tuned your weights such that your model learned high-level features specific to the dataset. This technique is usually recommended when the training dataset is large and very similar to the orginial dataset that the pre-trained model was trained on.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv_fr",
   "language": "python",
   "name": "cv_fr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
